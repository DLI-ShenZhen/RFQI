## Run the lapply in parallel
return( parLapply(cl, X=X, fun=FUN, ...) )
}, finally = {
## Stop the cluster
stopCluster(cl)
})
}
#' mclapply
#'
#' If the OS is Windows, set mclapply to the
#' the hackish version. Otherwise, leave the
#' definition alone.
#' @export
mclapply <- switch( Sys.info()[['sysname']],
Windows = {mclapply.hack},
Linux   = {mclapply},
Darwin  = {mclapply})
mclapply
peaks
group_FT = mclapply(1:nrow(peaks), group_FT.FUN, df=peaks, absMz=absMz, absRt=absRt, mc.cores=cores)
cores
group_FT = mclapply(1:nrow(peaks), group_FT.FUN, df=peaks, absMz=absMz, absRt=absRt, mc.cores=1)
dim(peaks)
peaks = do.call(rbind, split_data[1:5])
startTime = Sys.time()
# group_FT.split_data = lapply(split_data, getPeakRange_singFile, cl=cl, param=param)
group_FT = mclapply(1:nrow(peaks), group_FT.FUN, df=peaks, absMz=absMz, absRt=absRt, mc.cores=2)
endTime = Sys.time()
duration = (as.numeric(endTime) - as.numeric(startTime))/60
print(sprintf("group %s peaks divided into %s part cost %s minutes", nrow(peaks_origin_sort), length(split_data), duration))
startTime = Sys.time()
# group_FT.split_data = lapply(split_data, getPeakRange_singFile, cl=cl, param=param)
group_FT = mclapply(1:nrow(peaks), group_FT.FUN, df=peaks, absMz=absMz, absRt=absRt, mc.cores=1)
endTime = Sys.time()
duration = (as.numeric(endTime) - as.numeric(startTime))/60
print(sprintf("group %s peaks divided into %s part cost %s minutes", nrow(peaks_origin_sort), length(split_data), duration))
dim(peaks)
peaks = do.call(rbind, split_data[1:10])
startTime = Sys.time()
# group_FT.split_data = lapply(split_data, getPeakRange_singFile, cl=cl, param=param)
group_FT = mclapply(1:nrow(peaks), group_FT.FUN, df=peaks, absMz=absMz, absRt=absRt, mc.cores=2)
endTime = Sys.time()
duration = (as.numeric(endTime) - as.numeric(startTime))/60
print(sprintf("group %s peaks divided into %s part cost %s minutes", nrow(peaks_origin_sort), length(split_data), duration))
peaks
dim(peaks)
if (!("id" %in% colnames(peaks))){peaks = cbind(peaks, id=1:nrow(peaks))}
head(peaks)
peaks = do.call(rbind, split_data[1:10])
if (!("id" %in% colnames(peaks))){peaks = cbind(peaks, id=1:nrow(peaks))}
or = order(as.numeric(peaks[,"mzmin"]))
peaks = peaks[or,]
startTime = Sys.time()
# group_FT.split_data = lapply(split_data, getPeakRange_singFile, cl=cl, param=param)
group_FT = mclapply(1:nrow(peaks), group_FT.FUN, df=peaks, absMz=absMz, absRt=absRt, mc.cores=2)
endTime = Sys.time()
duration = (as.numeric(endTime) - as.numeric(startTime))/60
print(sprintf("group %s peaks divided into %s part cost %s minutes", nrow(peaks_origin_sort), length(split_data), duration))
dim(peaks)
rm(x)
rm(list = ls())
stop(cl)
environment()
devtools::uninstall()
devtools::install()
## ------------------------------------------------------------------------
library(RFQI)
options(stringsAsFactors = TRUE)
## ---- include=TRUE-------------------------------------------------------
files <- dir(system.file(package="RFQI", dir="extdata"),
full.name=TRUE,
pattern="mzXML$")
param = new("ParamSet", binSize=0.25, peakwidth=c(2,30), ppm=10, noise=0, absMz=0.005, absRt=15)
## ------------------------------------------------------------------------
refFile = files[1]
files = files[2]
mclapply
f.in = files
ref = refFile
param=param
cores=2
peaks=NULL
prefix="FT"
print("beginning to get peaks from each sample")
# get peak-range. if peaks is not null, return peaks; else extract peaks from multiple files
absMz = param@absMz
absRt = param@absRt
if(is.null(peaks)){
peaks_adjustRT = findPeaks_L(f.in = f.in, ref=ref,param=param)
} else{
peaks_adjustRT = peaks
}
print("beginning to group/combine peaks")
# ------ combined peaks found using centwave accoding to mz and rt overlap, result is peak_range --- #
if (!("id" %in% colnames(peaks_adjustRT))) {
peaks_origin = cbind(peaks_adjustRT, id=1:nrow(peaks_adjustRT))     # add an id for each peak
} else {
peaks_origin = peaks_adjustRT
}
or = order(as.numeric(peaks_origin[,"mzmin"]))   # sort peaks based on mzmin. Group peaks maily based on m/z.
peaks_origin_sort = peaks_origin[or,]            # This step is to ensure peaks only compare nearby peaks.
# split data for parallel computation
# small len will cost less time, e.g. when 1000 items split 4 part (len=250), group_FT cost 10s
# while len=2, 15s; len=1000, 28s
len = 250 # combine 14,000 peaks cost 16 minutes
sections = ceiling(nrow(peaks_origin_sort)/len)
f = rep(seq(1:sections), each=len)
f = f[1:nrow(peaks_origin_sort)]
split_data = split.data.frame(peaks_origin_sort, f=f)
# group peaks of each split, return combine_FT
startTime = Sys.time()
cores
# group_FT.split_data = lapply(split_data, getPeakRange_singFile, cl=cl, param=param)
group_FT.split_data = lapply(split_data[1:4], getPeakRange_singFile, cores=cores, param=param)
endTime = Sys.time()
duration = (as.numeric(endTime) - as.numeric(startTime))/60
print(sprintf("group %s peaks divided into %s part cost %s minutes", nrow(peaks_origin_sort), length(split_data), duration))
# group peaks of each split, return combine_FT
startTime = Sys.time()
# group_FT.split_data = lapply(split_data, getPeakRange_singFile, cl=cl, param=param)
group_FT.split_data = lapply(split_data[1:8], getPeakRange_singFile, cores=cores, param=param)
endTime = Sys.time()
duration = (as.numeric(endTime) - as.numeric(startTime))/60
print(sprintf("group %s peaks divided into %s part cost %s minutes", nrow(peaks_origin_sort), length(split_data), duration))
str(group_FT)
str(group_FT.split_data)
# group_FT.split_data = lapply(split_data, getPeakRange_singFile, cl=cl, param=param)
group_FT.split_data = lapply(split_data[1:8], getPeakRange_singFile, cores=1, param=param)
# group peaks of each split, return combine_FT
startTime = Sys.time()
# group_FT.split_data = lapply(split_data, getPeakRange_singFile, cl=cl, param=param)
group_FT.split_data = lapply(split_data[1:8], getPeakRange_singFile, cores=1, param=param)
endTime = Sys.time()
duration = (as.numeric(endTime) - as.numeric(startTime))/60
print(sprintf("group %s peaks divided into %s part cost %s minutes", nrow(peaks_origin_sort), length(split_data), duration))
str(group_FT.split_data)
devtools::uninstall()
devtools::install()
# group_FT.split_data = lapply(split_data, getPeakRange_singFile, cl=cl, param=param)
group_FT.split_data = lapply(split_data[1:20], getPeakRange_singFile, cores=cores, param=param)
library(RFQI)
mclapply
# group_FT.split_data = lapply(split_data, getPeakRange_singFile, cl=cl, param=param)
group_FT.split_data = lapply(split_data[1:20], getPeakRange_singFile, cores=cores, param=param)
# group_FT.split_data = lapply(split_data, getPeakRange_singFile, cl=cl, param=param)
group_FT.split_data = lapply(split_data[1:3], getPeakRange_singFile, cores=cores, param=param)
# group peaks of each split, return combine_FT
startTime = Sys.time()
# group_FT.split_data = lapply(split_data, getPeakRange_singFile, cl=cl, param=param)
group_FT.split_data = lapply(split_data[1:3], getPeakRange_singFile, cores=cores, param=param)
endTime = Sys.time()
duration = (as.numeric(endTime) - as.numeric(startTime))/60
print(sprintf("group %s peaks divided into %s part cost %s minutes", nrow(peaks_origin_sort), length(split_data), duration))
# group peaks of each split, return combine_FT
startTime = Sys.time()
# group_FT.split_data = lapply(split_data, getPeakRange_singFile, cl=cl, param=param)
group_FT.split_data = lapply(split_data[1:3], getPeakRange_singFile, cores=1, param=param)
endTime = Sys.time()
duration = (as.numeric(endTime) - as.numeric(startTime))/60
print(sprintf("group %s peaks divided into %s part cost %s minutes", nrow(peaks_origin_sort), length(split_data), duration))
# group peaks of each split, return combine_FT
startTime = Sys.time()
# group_FT.split_data = lapply(split_data, getPeakRange_singFile, cl=cl, param=param)
group_FT.split_data = lapply(split_data[1], getPeakRange_singFile, cores=1, param=param)
endTime = Sys.time()
duration = (as.numeric(endTime) - as.numeric(startTime))/60
print(sprintf("group %s peaks divided into %s part cost %s minutes", nrow(peaks_origin_sort), length(split_data), duration))
# group peaks of each split, return combine_FT
startTime = Sys.time()
# group_FT.split_data = lapply(split_data, getPeakRange_singFile, cl=cl, param=param)
group_FT.split_data = lapply(split_data[1], getPeakRange_singFile, cores=2, param=param)
endTime = Sys.time()
duration = (as.numeric(endTime) - as.numeric(startTime))/60
print(sprintf("group %s peaks divided into %s part cost %s minutes", nrow(peaks_origin_sort), length(split_data), duration))
# group peaks of each split, return combine_FT
startTime = Sys.time()
# group_FT.split_data = lapply(split_data, getPeakRange_singFile, cl=cl, param=param)
group_FT.split_data = lapply(split_data[1], getPeakRange_singFile, cores=3, param=param)
endTime = Sys.time()
duration = (as.numeric(endTime) - as.numeric(startTime))/60
print(sprintf("group %s peaks divided into %s part cost %s minutes", nrow(peaks_origin_sort), length(split_data), duration))
# group peaks of each split, return combine_FT
startTime = Sys.time()
# group_FT.split_data = lapply(split_data, getPeakRange_singFile, cl=cl, param=param)
group_FT.split_data = lapply(split_data[1:8], getPeakRange_singFile, cores=3, param=param)
endTime = Sys.time()
duration = (as.numeric(endTime) - as.numeric(startTime))/60
print(sprintf("group %s peaks divided into %s part cost %s minutes", nrow(peaks_origin_sort), length(split_data), duration))
# group peaks of each split, return combine_FT
startTime = Sys.time()
# group_FT.split_data = lapply(split_data, getPeakRange_singFile, cl=cl, param=param)
group_FT.split_data = lapply(split_data[1:8], getPeakRange_singFile, cores=2, param=param)
endTime = Sys.time()
duration = (as.numeric(endTime) - as.numeric(startTime))/60
print(sprintf("group %s peaks divided into %s part cost %s minutes", nrow(peaks_origin_sort), length(split_data), duration))
# group peaks of each split, return combine_FT
startTime = Sys.time()
# group_FT.split_data = lapply(split_data, getPeakRange_singFile, cl=cl, param=param)
group_FT.split_data = lapply(split_data[1:8], getPeakRange_singFile, cores=1, param=param)
endTime = Sys.time()
duration = (as.numeric(endTime) - as.numeric(startTime))/60
print(sprintf("group %s peaks divided into %s part cost %s minutes", nrow(peaks_origin_sort), length(split_data), duration))
cores
# group peaks of each split, return combine_FT
startTime = Sys.time()
# group_FT.split_data = lapply(split_data, getPeakRange_singFile, cl=cl, param=param)
group_FT.split_data = lapply(split_data, getPeakRange_singFile, cores=cores, param=param)
endTime = Sys.time()
duration = (as.numeric(endTime) - as.numeric(startTime))/60
print(sprintf("group %s peaks divided into %s part cost %s minutes", nrow(peaks_origin_sort), length(split_data), duration))
devtools::uninstall()
devtools::install()
environment()
a = function(){environment()}
a()
a = function(){parent.env()}
a()
a = function(){parent.env(environment())}
a()
a = environment()
environment()
a = environment()
identical(a, globalenv())
globalenv()
a = function(){globalenv()}
a()
#' registerPackage
#'
#' register packages into cluster
registerPackage = function(cl){
pkg.names <- c(
## Base packages
sessionInfo()$basePkgs,
## Additional packages
names( sessionInfo()$otherPkgs ))
lapply(pkg.names, function(x){
clusterEvalQ(cl, require(x))
})
}
#' registerParentVars
#'
#' register varlist of parent environment and current environment into cluster
registerParentVars = function(cl){
this.env <- environment()
if (this.env != globalenv()){
# register current environment variables
clusterExport(cl,
ls(all.names = TRUE, env=this.env),
envir = this.env)
# search parent environment
this.env = parent.env(environment())
# register parent environment variables
clusterExport(cl,
ls(all.names = TRUE, env=this.env),
envir = this.env)
}
# ensure register globaleEnv varlist
if (this.env != globalenv()){
clusterExport(cl,
ls(all.names=TRUE, env=globalenv()),
envir=globalenv())
}
}
#' Define the hack
#'
mclapply.hack <- function(X, FUN, mc.cores, ...) {
cores = mc.cores
cl <- makeCluster( min(mc.cores, availableCores()-1) )
tryCatch({
registerPackage(cl)
registerParentVars(cl)
## Run the lapply in parallel
return( parLapply(cl, X=X, fun=FUN, ...) )
}, finally = {
## Stop the cluster
stopCluster(cl)
})
}
#' mclapply
#'
#' If the OS is Windows, set mclapply to the
#' the hackish version. Otherwise, leave the
#' definition alone.
#' @export
mclapply <- switch( Sys.info()[['sysname']],
Windows = {mclapply.hack},
Linux   = {mclapply},
Darwin  = {mclapply})
devtools::document()
devtools::uninstall()
devtools::install()
files
refFile
## ------------------------------------------------------------------------
features = getPeakRange_multipleFiles(f.in = files, ref = refFile, param=param, cores=2)
library(RFQI)
## ------------------------------------------------------------------------
features = getPeakRange_multipleFiles(f.in = files, ref = refFile, param=param, cores=2)
devtools::uninstall()
devtools::install()
library(RFQI)
## ------------------------------------------------------------------------
features = getPeakRange_multipleFiles(f.in = files, ref = refFile, param=param, cores=2)
devtools::uninstall()
devtools::install()
library(RFQI)
## ------------------------------------------------------------------------
features = getPeakRange_multipleFiles(f.in = files, ref = refFile, param=param, cores=2)
ls("RFQI")
env.profile("RFQI")
library(devtools)
library(devtools)
rm(list = ls())
devtools::document()
devtools::uninstall()
devtools::install()
## ------------------------------------------------------------------------
library(RFQI)
options(stringsAsFactors = TRUE)
## ---- include=TRUE-------------------------------------------------------
files <- dir(system.file(package="RFQI", dir="extdata"),
full.name=TRUE,
pattern="mzXML$")
param = new("ParamSet", binSize=0.25, peakwidth=c(2,30), ppm=10, noise=0, absMz=0.005, absRt=15)
## ------------------------------------------------------------------------
refFile = files[1]
files = files[-1]
## ------------------------------------------------------------------------
features = getPeakRange_multipleFiles(f.in = files, ref = refFile, param=param, cores=2)
devtools::uninstall()
devtools::install()
library(RFQI)
## ------------------------------------------------------------------------
features = getPeakRange_multipleFiles(f.in = files, ref = refFile, param=param, cores=2)
str(features)
## ------------------------------------------------------------------------
str(features, max.level = 1)
## ------------------------------------------------------------------------
files <- dir(system.file(package="RFQI", dir="extdata"), full.name=TRUE, pattern="mzXML$")
MS2 = getMS2FromFiles(files=files, ref_file = refFile)
str(MS2, max.level = 1)
## ------------------------------------------------------------------------
mzCount = unlist(lapply(MS2$MS2, nrow)) # A qualified MS2 spectrum should have at least 3 fragments
mzMaxInt = unlist(lapply(MS2$MS2, function(x){max(x[,2])}))  # A qualified MS2 spectrum should have at least one fragment whose intensity greater than 32 (user defined)
CountFlag = !(mzCount %in% c(0,1,2))
IntFlag = mzMaxInt > 50
flag = CountFlag & IntFlag
MS2$MS2 = MS2$MS2[flag]
MS2$precursorInfo = MS2$precursorInfo[flag,]
## ------------------------------------------------------------------------
names(MS2$MS2) = rownames(MS2$precursorInfo) = sapply(1:nrow(MS2$precursorInfo),
convert_num_to_char, prefix = "M", n=nchar(nrow(MS2$precursorInfo)))
## ------------------------------------------------------------------------
db.MS2 = align_MS2_to_MS1(ms2Info = MS2, features = features)
## ------------------------------------------------------------------------
# we can calcalate similarity distribution for one feature
idx = getFeatureHasMS2(MS2DB=db.MS2, n=2)  # n is the minimum number of MS2 belonging to the same feature
distri = get_ms2Cor_inner(MS2DB = db.MS2, cores=2, maxMS2 = 100, idx=idx[1])
## ------------------------------------------------------------------------
plot(density(distri[[1]][,1], na.rm=TRUE), main="MS2 similarity distribution", col="red", xlab="cosine similarity score")
for (i in 2:nrow(distri[[1]])){
lines(density(distri[[1]][,i], na.rm=TRUE), col="red")
}
## ------------------------------------------------------------------------
db.MS2.subset = list(MS2=db.MS2$MS2[1:500], MS2_to_MS1=db.MS2$MS2_to_MS1[1:500,])
ms2InnerCor = get_ms2Cor_inner(MS2DB = db.MS2.subset, cores = 2, n=2, maxMS2 = 100)
## ------------------------------------------------------------------------
data("spectrumDB", package = "RFQI")
ms2InnerCor
str(ms2InnerCor)
str(ms2InnerCor, max.level = 1)
str(spectrumDB, max.level = 1)
## ------------------------------------------------------------------------
anno = get_identify(lib = spectrumDB, MS2DB = db.MS2.subset, MS2_inner_cor = ms2InnerCor, absMz = 0.045, adduct="RP_pos", cores=2)
anno
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>",
warning = FALSE,
message=FALSE
)
install.packages("knitr")
str(anno, max.level = 1)
## ------------------------------------------------------------------------
result = list()
for (file in files){
result[[file]] = getPeaks_L(file = file, peak_range = features$group_FT, ref_file = refFile, MS1=TRUE, step=0.1, MS2=FALSE)$peaks[,"into"]
}
intensity = do.call(cbind,result)
colnames(intensity) = names(result)
str(result, max.level = 1)
head(result)
head(features$group_FT)
dim(result[[1]])
## ------------------------------------------------------------------------
result = list()
for (file in files){
result[[file]] = getPeaks_L(file = file, peak_range = features$group_FT, ref_file = refFile, MS1=TRUE, step=0.1, MS2=FALSE)$peaks[,"into"]
}
str(result)
intensity = do.call(cbind,result)
colnames(intensity) = names(result)
dim(intensity)
head(intensity)
str(features$group_FT)
result = list()
for (file in files){
result[[file]] = getPeaks_L(file = file, peak_range = features$group_FT, ref_file = refFile, MS1=TRUE, step=0.1, MS2=FALSE)$peaks[,"into",drop=FALSE]
}
files
result = list()
for (file in files){
result[[file]] = getPeaks_L(file = file, peak_range = features$group_FT, ref_file = refFile, MS1=TRUE, step=0.1, MS2=FALSE)$peaks[,"into",drop=FALSE]
}
intensity = do.call(cbind,result)
colnames(intensity) = names(result)
str(result, max.level = 1)
head(intensity)
str(result)
str(result[[1]])
result = list()
for (file in files){
result[[file]] = getPeaks_L(file = file, peak_range = features$group_FT, ref_file = refFile, MS1=TRUE, step=0.1, MS2=FALSE) # $peaks[,"into",drop=FA]
}
str(result)
str(result[[1]]$peaks)
features$group_FT
dim(features$group_FT)
devtools::document()
devtools::uninstall()
devtools::build_vignettes()
install.packages("rmarkdown")
devtools::build_vignettes()
library(rmarkdown)
devtools::build_vignettes()
devtools::build_vignettes()
devtools::build_vignettes()
devtools::document()
devtools::uninstall()
devtools::install()
devtools::build_vignettes()
```{r setup, include = FALSE}
rm(list = ls())
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>",
warning = FALSE,
message=FALSE
)
```{r setup, include = FALSE}
library(RFQI)
options(stringsAsFactors = FALSE)
files <- dir(system.file(package="RFQI", dir="extdata"),
full.name=TRUE,
pattern="mzXML$")
param = new("ParamSet", binSize=0.25, peakwidth=c(2,30), ppm=10, noise=0, absMz=0.005, absRt=15)
refFile = files[1]
files = files[-1]
files <- dir(system.file(package="RFQI", dir="extdata"),
full.name=TRUE,
pattern="mzXML$")
refFile = files[1]
files = files[1]
features = getPeakRange_multipleFiles(f.in = files, ref = refFile, param=param, cores=2)
files <- dir(system.file(package="RFQI", dir="extdata"),
full.name=TRUE,
pattern="mzXML$")
refFile = files[1]
files = files[2]
features = getPeakRange_multipleFiles(f.in = files, ref = refFile, param=param, cores=2)
str(features, max.level = 1)
files <- dir(system.file(package="RFQI", dir="extdata"), full.name=TRUE, pattern="mzXML$")
MS2 = getMS2FromFiles(files=files, ref_file = refFile)
str(MS2, max.level = 1)
mzCount = unlist(lapply(MS2$MS2, nrow)) # A qualified MS2 spectrum should have at least 3 fragments
mzMaxInt = unlist(lapply(MS2$MS2, function(x){max(x[,2])}))  # A qualified MS2 spectrum should have at least one fragment whose intensity greater than 32 (user defined)
CountFlag = !(mzCount %in% c(0,1,2))
IntFlag = mzMaxInt > 50
flag = CountFlag & IntFlag
MS2$MS2 = MS2$MS2[flag]
MS2$precursorInfo = MS2$precursorInfo[flag,]
names(MS2$MS2) = rownames(MS2$precursorInfo) = sapply(1:nrow(MS2$precursorInfo),
convert_num_to_char, prefix = "M", n=nchar(nrow(MS2$precursorInfo)))
names(MS2$MS2) = rownames(MS2$precursorInfo) = sapply(1:nrow(MS2$precursorInfo),
convert_num_to_char, prefix = "M", n=nchar(nrow(MS2$precursorInfo)))
db.MS2 = align_MS2_to_MS1(ms2Info = MS2, features = features)
# we can calcalate similarity distribution for one feature
idx = getFeatureHasMS2(MS2DB=db.MS2, n=2)  # n is the minimum number of MS2 belonging to the same feature
distri = get_ms2Cor_inner(MS2DB = db.MS2, cores=2, maxMS2 = 100, idx=idx[1])
plot(density(distri[[1]][,1], na.rm=TRUE), main="MS2 similarity distribution", col="red", xlab="cosine similarity score")
for (i in 2:nrow(distri[[1]])){
lines(density(distri[[1]][,i], na.rm=TRUE), col="red")
}
plot(density(distri[[1]][,1], na.rm=TRUE), main="MS2 similarity distribution", col="red", xlab="cosine similarity score")
for (i in 2:nrow(distri[[1]])){
lines(density(distri[[1]][,i], na.rm=TRUE), col="red")
}
db.MS2.subset = list(MS2=db.MS2$MS2[1:500], MS2_to_MS1=db.MS2$MS2_to_MS1[1:500,]) # this line is only for test
ms2InnerCor = get_ms2Cor_inner(MS2DB = db.MS2,cores = 2, n=2, maxMS2 = 100)
ms2InnerCor = get_ms2Cor_inner(MS2DB = db.MS2.subset,cores = 2, n=2, maxMS2 = 100)
data("spectrumDB", package = "RFQI")
str(spectrumDB, max.level = 1)
anno = get_identify(lib = spectrumDB, MS2DB = db.MS2.subset, MS2_inner_cor = ms2InnerCor, absMz = 0.045, adduct="RP_pos", cores=3)
str(anno)
str(anno, max.level = 1)
str(anno$anno, max.level = 1)
anno$anno
devtools::uninstall()
devtools::document()
devtools::build_vignettes()
future::availableCores()
library(future)
